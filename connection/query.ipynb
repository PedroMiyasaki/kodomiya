{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Criar conexão\n",
    "con = duckdb.connect(database=r\"C:\\Users\\PedroMiyasaki\\OneDrive - DHAUZ\\Área de Trabalho\\Projetos\\PESSOAL\\kodomiya\\kodomiya\\db\\kodomiya.duckdb\")\n",
    "\n",
    "kb_querys = [\n",
    "    \"SELECT preco, tamanho, n_quartos, n_banheiros, n_garagem, bairro, latitude, longitude, id FROM kodomiya_chaves_na_mao.chaves_na_mao_register\",\n",
    "    \"SELECT preco, tamanho, n_quartos, n_banheiros, n_garagem, bairro, latitude, longitude, id FROM kodomiya_viva_real.viva_real_register\",\n",
    "    \"SELECT preco, tamanho, n_quartos, n_banheiros, n_garagem, bairro, latitude, longitude, id FROM kodomiya_zap_imoveis.zap_imoveis_register\"\n",
    "]\n",
    "\n",
    "# Fetch knowledge base data\n",
    "df_kb_list = [con.execute(query).fetch_df() for query in kb_querys]\n",
    "\n",
    "# Concatenate knowledge base data\n",
    "df_kb = pd.concat(df_kb_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x1dae65082f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Criar conexão\n",
    "con = duckdb.connect(database=r\"C:\\Users\\PedroMiyasaki\\OneDrive - DHAUZ\\Área de Trabalho\\Projetos\\PESSOAL\\kodomiya\\kodomiya\\db\\kodomiya.duckdb\")\n",
    "\n",
    "# Definir a quaery\n",
    "query = \"\"\"\n",
    "DROP SCHEMA kodomiya_leilao_imovel CASCADE;\n",
    "\"\"\"\n",
    "\n",
    "#con.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from datetime import datetime\n",
    "\n",
    "# Criar conexão\n",
    "con = duckdb.connect(database=r\"C:\\Users\\PedroMiyasaki\\OneDrive - DHAUZ\\Área de Trabalho\\Projetos\\PESSOAL\\kodomiya\\kodomiya\\db\\kodomiya.duckdb\")\n",
    "\n",
    "# Executar uma query no banco\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "df_leilao = con.execute(f\"SELECT * FROM kodomiya_leilao_imovel.leilao_imovel_register WHERE data_primeira_praca\t< '{today}' OR data_segunda_praca < '{today}'\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bairro</th>\n",
       "      <th>rua</th>\n",
       "      <th>cidade</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>umbara</td>\n",
       "      <td>RUA DILSON LUIZ</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uberaba</td>\n",
       "      <td>RUA RODOLFO BERNARDELLI</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>-25.489124</td>\n",
       "      <td>-49.226351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bairro                      rua    cidade   latitude  longitude\n",
       "0   umbara          RUA DILSON LUIZ  Curitiba        NaN        NaN\n",
       "1  uberaba  RUA RODOLFO BERNARDELLI  Curitiba -25.489124 -49.226351"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_leilao[[\"bairro\", \"rua\", \"cidade\", \"latitude\", \"longitude\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bairro\n",
       "umbara     1\n",
       "uberaba    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_leilao[\"bairro\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from datetime import datetime\n",
    "\n",
    "# Criar conexão\n",
    "con = duckdb.connect(database=r\"C:\\Users\\PedroMiyasaki\\OneDrive - DHAUZ\\Área de Trabalho\\Projetos\\PESSOAL\\kodomiya\\kodomiya\\db\\kodomiya.duckdb\")\n",
    "\n",
    "# Executar uma query no banco\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "df_leilao = con.execute(f\"SELECT * FROM kodomiya_leilao_imovel.leilao_imovel_register\").fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop imoveis older than oldest imovel on leilao table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from datetime import datetime\n",
    "\n",
    "# Criar conexão\n",
    "con = duckdb.connect(database=r\"C:\\Users\\PedroMiyasaki\\OneDrive - DHAUZ\\Área de Trabalho\\Projetos\\PESSOAL\\kodomiya\\kodomiya\\db\\kodomiya.duckdb\")\n",
    "\n",
    "# Read leilao table\n",
    "df_leilao = con.execute(f\"SELECT * FROM kodomiya_leilao_imovel.leilao_imovel_register\").fetchdf()\n",
    "\n",
    "# Take min_date\n",
    "min_date = df_leilao[\"datahora\"].min().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x189f32b4830>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_query = f\"\"\"\n",
    "    DELETE FROM kodomiya_zap_imoveis.zap_imoveis_history\n",
    "    WHERE datahora < '{min_date}'\n",
    "\"\"\"\n",
    "\n",
    "# con.execute(del_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Criar conexão\n",
    "con = duckdb.connect(database=r\"example.duckdb\")\n",
    "                  \n",
    "# Select all columns needed for knowledge base\n",
    "kb_querys = [\n",
    "    \"SELECT * FROM kodomiya_chaves_na_mao.chaves_na_mao_register\",\n",
    "    \"SELECT * FROM kodomiya_viva_real.viva_real_register\",\n",
    "    \"SELECT * FROM kodomiya_zap_imoveis.zap_imoveis_register\"\n",
    "]\n",
    "\n",
    "# Fetch knowledge base data\n",
    "df_kb_list = [con.execute(query).fetch_df() for query in kb_querys]\n",
    "\n",
    "# Concatenate knowledge base data\n",
    "df_kb = pd.concat(df_kb_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'datahora', 'preco', 'tamanho', 'n_quartos', 'n_banheiros',\n",
       "       'n_garagem', 'rua', 'bairro', 'cidade', 'latitude', 'longitude',\n",
       "       '_dlt_load_id', '_dlt_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kb.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standalone exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from unidecode import unidecode\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-04 11:03:54,421 - run_pre_analysis - INFO - Loaded 5168 properties from knowledge base and 89 open auctions.\n",
      "2025-07-04 11:03:54,443 - run_pre_analysis - INFO - --- Imputing coordinates for Knowledge Base ---\n",
      "2025-07-04 11:03:54,445 - run_pre_analysis - INFO - Starting lat/lon imputation. Nulls before: Lat=48, Lon=48\n",
      "2025-07-04 11:03:54,447 - run_pre_analysis - INFO - Using KB for imputation. City-wide mean: Lat=-25.4522, Lon=-49.2720\n",
      "2025-07-04 11:03:54,453 - run_pre_analysis - INFO - Lat/lon imputation complete. Nulls after: Lat=0, Lon=0\n",
      "2025-07-04 11:03:54,454 - run_pre_analysis - INFO - --- Imputing coordinates for Auction Data ---\n",
      "2025-07-04 11:03:54,455 - run_pre_analysis - INFO - Starting lat/lon imputation. Nulls before: Lat=3, Lon=3\n",
      "2025-07-04 11:03:54,457 - run_pre_analysis - INFO - Using KB for imputation. City-wide mean: Lat=-25.4528, Lon=-49.2722\n",
      "2025-07-04 11:03:54,464 - run_pre_analysis - INFO - Lat/lon imputation complete. Nulls after: Lat=0, Lon=0\n",
      "2025-07-04 11:03:54,464 - run_pre_analysis - INFO - Starting Z-Score based analysis...\n",
      "2025-07-04 11:03:54,471 - run_pre_analysis - INFO - 8 properties missing neighborhood stats, using city-wide fallback...\n",
      "2025-07-04 11:03:54,472 - run_pre_analysis - INFO - Fallback stats: Mean m² Price=$6776.16, Std Dev=$3171.27\n",
      "2025-07-04 11:03:54,478 - run_pre_analysis - INFO - Found 33 potentially undervalued properties (Z-score).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PedroMiyasaki\\AppData\\Local\\Temp\\ipykernel_22088\\842591247.py:162: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_leilao = df_leilao.applymap(lambda x: None if str(x) == \"<NA>\" else x)\n",
      "C:\\Users\\PedroMiyasaki\\AppData\\Local\\Temp\\ipykernel_22088\\842591247.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['latitude'].fillna(city_mean_lat, inplace=True)\n",
      "C:\\Users\\PedroMiyasaki\\AppData\\Local\\Temp\\ipykernel_22088\\842591247.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['longitude'].fillna(city_mean_lon, inplace=True)\n",
      "C:\\Users\\PedroMiyasaki\\AppData\\Local\\Temp\\ipykernel_22088\\842591247.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['latitude'].fillna(city_mean_lat, inplace=True)\n",
      "C:\\Users\\PedroMiyasaki\\AppData\\Local\\Temp\\ipykernel_22088\\842591247.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['longitude'].fillna(city_mean_lon, inplace=True)\n",
      "C:\\Users\\PedroMiyasaki\\AppData\\Local\\Temp\\ipykernel_22088\\842591247.py:252: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_leilao_zscore['mean_preco_m2'].fillna(city_mean_preco_m2, inplace=True)\n",
      "C:\\Users\\PedroMiyasaki\\AppData\\Local\\Temp\\ipykernel_22088\\842591247.py:253: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_leilao_zscore['std_preco_m2'].fillna(city_std_preco_m2, inplace=True)\n",
      "C:\\Users\\PedroMiyasaki\\AppData\\Local\\Temp\\ipykernel_22088\\842591247.py:286: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_opportunities['z_score_rank'].fillna(999, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "SELIC_RATE_ANNUAL = 14.50 / 100  # Annual SELIC rate (14.50%)\n",
    "\n",
    "def impute_lat_lon(df_to_impute, df_knowledge_base, logger):\n",
    "    \"\"\"\n",
    "    Imputes missing lat/lon in a dataframe using means from a knowledge base dataframe.\n",
    "    If imputing a dataframe on itself, pass it for both arguments.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting lat/lon imputation. Nulls before: Lat={df_to_impute['latitude'].isnull().sum()}, Lon={df_to_impute['longitude'].isnull().sum()}\")\n",
    "    \n",
    "    df = df_to_impute.copy()\n",
    "\n",
    "    # 1. Calculate means from the knowledge base\n",
    "    city_mean_lat = df_knowledge_base['latitude'].mean()\n",
    "    city_mean_lon = df_knowledge_base['longitude'].mean()\n",
    "\n",
    "    if not (pd.notna(city_mean_lat) and pd.notna(city_mean_lon)):\n",
    "        logger.warning(\"Could not calculate city-wide mean coordinates from knowledge base. Skipping imputation.\")\n",
    "        df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "        return df\n",
    "\n",
    "    logger.info(f\"Using KB for imputation. City-wide mean: Lat={city_mean_lat:.4f}, Lon={city_mean_lon:.4f}\")\n",
    "    bairro_lat_map = df_knowledge_base.groupby('bairro')['latitude'].mean()\n",
    "    bairro_lon_map = df_knowledge_base.groupby('bairro')['longitude'].mean()\n",
    "    \n",
    "    # 2. Map bairro-specific means for rows with missing lat/lon\n",
    "    lat_na_mask = df['latitude'].isna()\n",
    "    if lat_na_mask.any():\n",
    "        df.loc[lat_na_mask, 'latitude'] = df.loc[lat_na_mask, 'bairro'].map(bairro_lat_map)\n",
    "\n",
    "    lon_na_mask = df['longitude'].isna()\n",
    "    if lon_na_mask.any():\n",
    "        df.loc[lon_na_mask, 'longitude'] = df.loc[lon_na_mask, 'bairro'].map(bairro_lon_map)\n",
    "    \n",
    "    # 3. Fallback to city-wide mean for any remaining NaNs\n",
    "    df['latitude'].fillna(city_mean_lat, inplace=True)\n",
    "    df['longitude'].fillna(city_mean_lon, inplace=True)\n",
    "\n",
    "    # 4. Final drop for safety\n",
    "    df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "    \n",
    "    logger.info(f\"Lat/lon imputation complete. Nulls after: Lat={df['latitude'].isnull().sum()}, Lon={df['longitude'].isnull().sum()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_adjusted_roi(purchase_price, estimated_resale_value, logger):\n",
    "    \"\"\"\n",
    "    Calculates a more realistic ROI by subtracting fixed and variable costs,\n",
    "    including opportunity cost based on the SELIC rate.\n",
    "\n",
    "    Args:\n",
    "        purchase_price (float): The price paid for the property.\n",
    "        estimated_resale_value (float): The estimated value the property will be sold for.\n",
    "        logger (logging.Logger): The logger instance.\n",
    "\n",
    "    Returns:\n",
    "        float: The adjusted ROI as a percentage, or 0.0 if costs exceed profit.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Transactional Costs on Purchase\n",
    "        documentation_cost = purchase_price * 0.055\n",
    "        auctioneer_commission = purchase_price * 0.05\n",
    "        legal_fee = 5000  # Imissão na posse\n",
    "        other_costs = 300\n",
    "        \n",
    "        total_purchase_costs = documentation_cost + auctioneer_commission + legal_fee + other_costs\n",
    "        total_investment = purchase_price + total_purchase_costs\n",
    "\n",
    "        # 2. Transactional Costs on Resale\n",
    "        resale_broker_commission = estimated_resale_value * 0.06 # Using 6% as a standard broker fee\n",
    "\n",
    "        # 3. Opportunity Cost (over 10 months)\n",
    "        selic_10_months = ((1 + SELIC_RATE_ANNUAL)**(10/12)) - 1\n",
    "        opportunity_cost = total_investment * selic_10_months\n",
    "\n",
    "        # 4. Net Profit Calculation\n",
    "        gross_profit = estimated_resale_value - total_investment\n",
    "        net_profit = gross_profit - resale_broker_commission - opportunity_cost\n",
    "            \n",
    "        adjusted_roi = (net_profit / total_investment) * 100\n",
    "        \n",
    "        return adjusted_roi\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in calculate_adjusted_roi: {e}\", exc_info=True)\n",
    "        return 0.0\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Set up logging configuration.\"\"\"\n",
    "    logger = logging.getLogger(\"run_pre_analysis\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    \n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "def impute_leilao_data(df, imputers, logger):\n",
    "    \"\"\"Imputes missing values in auction data using pre-trained imputers.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    logger.info(\"Imputing missing values in auction data...\")\n",
    "\n",
    "    # Impute n_quartos and n_banheiros using the loaded Linear Regression models\n",
    "    for col in ['n_quartos', 'n_banheiros']:\n",
    "        if col not in df_copy.columns:\n",
    "            logger.warning(f\"Column '{col}' not found in auction data. Creating it for imputation.\")\n",
    "            df_copy[col] = np.nan\n",
    "            \n",
    "        imputer_model = imputers.get(f'{col}_imputer')\n",
    "        if imputer_model:\n",
    "            # Predict only for rows with missing values in the target column but valid 'tamanho'\n",
    "            missing_mask = df_copy[col].isna() & df_copy['tamanho'].notna()\n",
    "            if missing_mask.any():\n",
    "                # The imputer model was trained on 'tamanho'\n",
    "                features_for_imputation = df_copy.loc[missing_mask, ['tamanho']]\n",
    "                predicted_values = imputer_model.predict(features_for_imputation)\n",
    "                df_copy.loc[missing_mask, col] = np.round(predicted_values).clip(1)\n",
    "\n",
    "    # Impute n_garagem using the saved median value\n",
    "    if 'n_garagem' not in df_copy.columns:\n",
    "        df_copy['n_garagem'] = np.nan\n",
    "    garagem_imputer = imputers.get('n_garagem_imputer')\n",
    "    if garagem_imputer is not None:\n",
    "        df_copy['n_garagem'].fillna(garagem_imputer, inplace=True)\n",
    "\n",
    "    # Fill any remaining NaNs after imputation (e.g., if area_util was NaN) with the median\n",
    "    for col in ['n_quartos', 'n_banheiros', 'n_garagem']:\n",
    "        if col in df_copy.columns and df_copy[col].isna().any():\n",
    "            fallback_median = imputers.get('n_garagem_imputer', 1) # Use garage median or 1 as a fallback\n",
    "            df_copy[col].fillna(fallback_median, inplace=True)\n",
    "        # Ensure integer types\n",
    "        df_copy[col] = df_copy[col].astype(int)\n",
    "        \n",
    "    logger.info(\"Auction data imputation complete.\")\n",
    "    return df_copy\n",
    "\n",
    "def kb_data_cleaning(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the knowledge base dataframe.\n",
    "    \"\"\"\n",
    "    df = df.drop_duplicates(subset=[\"id\"])\n",
    "    df = df.loc[(df[\"n_quartos\"] > 0) & (df[\"n_quartos\"] < 10)]\n",
    "    df = df.loc[(df[\"n_banheiros\"] > 0) & (df[\"n_banheiros\"] < 10)]\n",
    "    df = df.loc[(df[\"tamanho\"] > 10) & (df[\"tamanho\"] < 1000)]\n",
    "    df = df.loc[(df[\"preco\"] > 10_000) & (df[\"preco\"] < 10_000_000)]\n",
    "    df = df.loc[df[\"bairro\"].notna()]\n",
    "    df[\"bairro\"] = df[\"bairro\"].apply(lambda x: unidecode(x.lower().strip()))\n",
    "    df[\"preco_m2\"] = df[\"preco\"] / df[\"tamanho\"]\n",
    "    df = df.loc[df[\"preco_m2\"] < 30_000]\n",
    "    return df\n",
    "\n",
    "def leilao_data_cleaning(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the auction dataframe based on the new schema.\n",
    "    \"\"\"\n",
    "    df_leilao = df.copy()\n",
    "    df_leilao = df_leilao.applymap(lambda x: None if str(x) == \"<NA>\" else x)\n",
    "    df_leilao = df_leilao.drop_duplicates(subset=[\"id\"])\n",
    "\n",
    "    # Just maintain properties that acept financing (or are not available)\n",
    "    df_leilao[\"aceita_financiamento\"] = df_leilao[\"aceita_financiamento\"].apply(str).fillna(\"N/A\")\n",
    "    df_leilao = df_leilao.loc[(df_leilao[\"aceita_financiamento\"] == \"True\") | (df_leilao[\"aceita_financiamento\"] == \"N/A\")]\n",
    "\n",
    "    # Rename columns to match the historical schema used across the script\n",
    "    df_leilao = df_leilao.rename(columns={\n",
    "        'preco_primeira_praca': 'preco',\n",
    "        'area_util': 'tamanho',\n",
    "        'link_detalhes': 'url'\n",
    "    })\n",
    "\n",
    "    # Ensure essential numeric columns are correctly typed, coercing errors to NaN\n",
    "    for col in ['preco', 'tamanho', 'preco_atual']:\n",
    "        if col in df_leilao.columns:\n",
    "            df_leilao[col] = pd.to_numeric(df_leilao[col], errors='coerce')\n",
    "    \n",
    "    # Drop rows that are unusable for analysis due to missing core information\n",
    "    df_leilao.dropna(subset=['preco', 'tamanho', 'bairro', 'preco_atual'], inplace=True)\n",
    "\n",
    "    # Calculate price per square meter\n",
    "    df_leilao['preco_m2'] = df_leilao['preco'] / df_leilao['tamanho']\n",
    "\n",
    "    # Standardize neighborhood names\n",
    "    df_leilao[\"bairro\"] = df_leilao[\"bairro\"].apply(lambda x: unidecode(str(x).lower().strip()))\n",
    "    return df_leilao\n",
    "\n",
    "\n",
    "# Database connection\n",
    "con = duckdb.connect(database=r\"C:\\Users\\PedroMiyasaki\\OneDrive - DHAUZ\\Área de Trabalho\\Projetos\\PESSOAL\\kodomiya\\kodomiya\\db\\kodomiya.duckdb\")\n",
    "\n",
    "# Select all columns needed for knowledge base\n",
    "kb_querys = [\n",
    "    \"SELECT preco, tamanho, n_quartos, n_banheiros, n_garagem, bairro, latitude, longitude, id FROM kodomiya_chaves_na_mao.chaves_na_mao_register\",\n",
    "    \"SELECT preco, tamanho, n_quartos, n_banheiros, n_garagem, bairro, latitude, longitude, id FROM kodomiya_viva_real.viva_real_register\",\n",
    "    \"SELECT preco, tamanho, n_quartos, n_banheiros, n_garagem, bairro, latitude, longitude, id FROM kodomiya_zap_imoveis.zap_imoveis_register\"\n",
    "]\n",
    "\n",
    "# Fetch knowledge base data\n",
    "df_kb_list = [con.execute(query).fetch_df() for query in kb_querys]\n",
    "\n",
    "# Concatenate knowledge base data\n",
    "df_kb = pd.concat(df_kb_list, ignore_index=True)\n",
    "\n",
    "# Fetch leilao data\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "df_leilao = con.execute(f\"SELECT * FROM kodomiya_leilao_imovel.leilao_imovel_register WHERE data_primeira_praca\t> '{today}' OR data_segunda_praca > '{today}'\").fetchdf()\n",
    "\n",
    "logger.info(f\"Loaded {len(df_kb)} properties from knowledge base and {len(df_leilao)} open auctions.\")\n",
    "\n",
    "\n",
    "# Data Cleaning\n",
    "df_kb = kb_data_cleaning(df_kb)\n",
    "df_leilao = leilao_data_cleaning(df_leilao)\n",
    "\n",
    "# Impute Lat/Lon for both dataframes using the knowledge base as the source of truth\n",
    "logger.info(\"--- Imputing coordinates for Knowledge Base ---\")\n",
    "df_kb = impute_lat_lon(df_kb, df_kb, logger) # Impute KB on itself\n",
    "logger.info(\"--- Imputing coordinates for Auction Data ---\")\n",
    "df_leilao = impute_lat_lon(df_leilao, df_kb, logger) # Impute auction data using KB stats\n",
    "\n",
    "\n",
    "# A. Z-Score Based Analysis (Undervalued Properties)\n",
    "# ----------------------------------------------------------------\n",
    "logger.info(\"Starting Z-Score based analysis...\")\n",
    "df_leilao_zscore = df_leilao.copy()\n",
    "\n",
    "neighborhood_stats = df_kb.groupby('bairro').agg(\n",
    "    mean_preco_m2=('preco_m2', 'mean'),\n",
    "    std_preco_m2=('preco_m2', 'std')\n",
    ").reset_index()\n",
    "\n",
    "df_leilao_zscore = df_leilao_zscore.merge(neighborhood_stats, on='bairro', how='left')\n",
    "\n",
    "# Create a flag for when we fall back to city-wide stats\n",
    "df_leilao_zscore['z_score_fallback'] = df_leilao_zscore['mean_preco_m2'].isnull()\n",
    "\n",
    "# Get city-wide stats for fallback\n",
    "city_mean_preco_m2 = df_kb['preco_m2'].mean()\n",
    "city_std_preco_m2 = df_kb['preco_m2'].std()\n",
    "\n",
    "# Log if fallback is happening\n",
    "if df_leilao_zscore['z_score_fallback'].any():\n",
    "    num_fallbacks = df_leilao_zscore['z_score_fallback'].sum()\n",
    "    logger.info(f\"{num_fallbacks} properties missing neighborhood stats, using city-wide fallback...\")\n",
    "    logger.info(f\"Fallback stats: Mean m² Price=${city_mean_preco_m2:.2f}, Std Dev=${city_std_preco_m2:.2f}\")\n",
    "\n",
    "# Apply fallback for mean and std\n",
    "df_leilao_zscore['mean_preco_m2'].fillna(city_mean_preco_m2, inplace=True)\n",
    "df_leilao_zscore['std_preco_m2'].fillna(city_std_preco_m2, inplace=True)\n",
    "    \n",
    "# Also, replace std of 0 (which gives infinite z-score) with city-wide std\n",
    "# This can happen if a neighborhood has only one listing in the KB\n",
    "df_leilao_zscore.loc[df_leilao_zscore['std_preco_m2'] == 0, 'std_preco_m2'] = city_std_preco_m2\n",
    "\n",
    "df_leilao_zscore['z_score'] = (\n",
    "    (df_leilao_zscore['preco_m2'] - df_leilao_zscore['mean_preco_m2']) / \n",
    "        df_leilao_zscore['std_preco_m2']\n",
    ")\n",
    "\n",
    "undervalued_properties = df_leilao_zscore.copy()\n",
    "undervalued_properties.loc[:, 'z_score_rank'] = undervalued_properties['z_score'].rank(ascending=True)\n",
    "undervalued_properties.loc[:, 'estimated_market_value_zscore'] = undervalued_properties['mean_preco_m2'] * undervalued_properties['tamanho']\n",
    "\n",
    "undervalued_properties['z_score_roi'] = undervalued_properties.apply(\n",
    "    lambda row: calculate_adjusted_roi(row['preco_atual'], row['estimated_market_value_zscore'], logger),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "logger.info(f\"Found {len(undervalued_properties)} potentially undervalued properties (Z-score).\")\n",
    "\n",
    "all_opportunities = undervalued_properties[['id', 'preco_atual', 'bairro', 'tamanho', 'url', 'rua', 'z_score', 'z_score_rank', 'z_score_roi', 'z_score_fallback']].copy()\n",
    "    \n",
    "# Re-populate descriptive data for all opportunities to fix NaNs from outer merges\n",
    "analysis_cols = ['id', 'z_score', 'z_score_rank', 'z_score_roi', 'z_score_fallback']\n",
    "detail_cols = ['preco_atual', 'bairro', 'tamanho', 'url', 'rua', 'descricao']\n",
    "\n",
    "analysis_cols_present = [c for c in analysis_cols if c in all_opportunities.columns]\n",
    "detail_cols_present = ['id'] + [c for c in detail_cols if c in df_leilao.columns]\n",
    "\n",
    "all_opportunities = all_opportunities[analysis_cols_present].merge(df_leilao[detail_cols_present], on='id', how='left')\n",
    "    \n",
    "all_opportunities['z_score_rank'].fillna(999, inplace=True)\n",
    "all_opportunities.fillna({'z_score_roi': 0, 'z_score_fallback': False}, inplace=True)\n",
    "\n",
    "all_opportunities['final_score'] = all_opportunities['z_score_rank'].astype(int)\n",
    "\n",
    "final_opportunities = all_opportunities[\n",
    "    (all_opportunities['z_score_rank'] != 999)\n",
    "].sort_values(by='final_score', ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
